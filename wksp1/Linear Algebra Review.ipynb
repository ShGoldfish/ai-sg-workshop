{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra / Numpy Review\n",
    "\n",
    "Before we learn about how we can apply neural networks to predict breast cancer, we should review the necessary linear algebra. This notebook will teach you how to use NumPy for basic linear algrebra and data manipulations. First, import the required modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key operation a neural network does on its input is a series of matrix multiplications. For fully-connected neural networks like the ones we will be working with today, these matrix multiplications are just matrix-vector products.\n",
    "\n",
    "That is, an input vector is multiplied by a matrix (representing the weights of the neural network) to produce another vector, and this vector is multiplied by another matrix to produce another vector, and this continues until the final vector, which is the output of the neural network. Here's an example of matrix-vector multiplication using NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [1 2 3] \n",
      "\n",
      "Weights:\n",
      " [[ 1 -4  5]\n",
      " [-7  2  2]\n",
      " [ 3 -5  1]] \n",
      "\n",
      "Result: [ -4 -15  12]\n"
     ]
    }
   ],
   "source": [
    "input_vector = np.array([1, 2, 3])\n",
    "weights_matrix = np.array([[1, -4, 5], [-7, 2, 2], [3, -5, 1]])\n",
    "\n",
    "\n",
    "print(\"Input:\", input_vector, \"\\n\")\n",
    "print(\"Weights:\\n\", weights_matrix, \"\\n\")\n",
    "\n",
    "result = np.matmul(input_vector, weights_matrix)\n",
    "\n",
    "print(\"Result:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simplified neural network that takes in an input of size 1x3 and outputs one of size 1x3. What happens if we change the dimension of the weights matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [1 2 3] \n",
      "\n",
      "Weights:\n",
      " [[ 1 -4]\n",
      " [-7  2]\n",
      " [ 3 -5]] \n",
      "\n",
      "Result: [ -4 -15]\n"
     ]
    }
   ],
   "source": [
    "input_vector = np.array([1, 2, 3])\n",
    "weights_matrix = np.array([[1, -4], [-7, 2], [3, -5]])\n",
    "\n",
    "\n",
    "print(\"Input:\", input_vector, \"\\n\")\n",
    "print(\"Weights:\\n\", weights_matrix, \"\\n\")\n",
    "\n",
    "result = np.matmul(input_vector, weights_matrix)\n",
    "\n",
    "print(\"Result:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We changed the shape of the outputs! This is because we did matrix multiplication using 1x3 and 3x2 matrices, which gives us a resulting matrix of size 1x2. Manipulating the shape of our weight matrices is how we will change the number of neurons in each layer of our neural network.\n",
    "\n",
    "## Bias Matrix\n",
    "\n",
    "Neural networks also usually have a bias matrix in addition to a weight matrix. We add this bias matrix to the result of matrix multiplication, so that each neuron in our output is formed by taking a linear function of all of the input neurons and adding a bias term. This makes our model more flexible, so it can more accurately model complex functions between inputs and outputs. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [1 2 3] \n",
      "\n",
      "Weights:\n",
      " [[ 1 -4]\n",
      " [-7  2]\n",
      " [ 3 -5]] \n",
      "\n",
      "Bias: [5 2] \n",
      "\n",
      "Result: [  1 -13]\n"
     ]
    }
   ],
   "source": [
    "input_vector = np.array([1, 2, 3])\n",
    "weights_matrix = np.array([[1, -4], [-7, 2], [3, -5]])\n",
    "bias_matrix = np.array([5, 2])\n",
    "\n",
    "\n",
    "print(\"Input:\", input_vector, \"\\n\")\n",
    "print(\"Weights:\\n\", weights_matrix, \"\\n\")\n",
    "print(\"Bias:\", bias_matrix, \"\\n\")\n",
    "\n",
    "result = np.matmul(input_vector, weights_matrix) + bias_matrix\n",
    "\n",
    "print(\"Result:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the bias matrix has the same dimensions as the output, so in this case it is a vector. Compare this result with the result from the last cell to see how the bias changed the result! \n",
    "\n",
    "## Activation Function\n",
    "\n",
    "The last major operation a neural network does to turn input into output is apply an activation function to the results of the matrix operations we just described. This is the actual output of a layer of a neural network. The point of an activation function is again to increase the flexibility of our model, so that it can model non-linear functions (note that all the operations we previously described are linear). \n",
    "\n",
    "What the activation function actually does varies depending on the specific function we are using. Today, we will be making use of ReLU and Sigmoid activation functions, so we'll focus on those.\n",
    "\n",
    "### ReLU\n",
    "\n",
    "This is the rectified linear unit activation function, and it is very simple. It simply takes each value x in the layer and returns max(0, x). This is the shape of the activation function:\n",
    "\n",
    "![ReLU](http://cs231n.github.io/assets/nn1/relu.jpeg)\n",
    "\n",
    "We simply apply this function to every element in the vector resulting from our vector operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [1 2 3] \n",
      "\n",
      "Weights:\n",
      " [[ 1 -4]\n",
      " [-7  2]\n",
      " [ 3 -5]] \n",
      "\n",
      "Bias: [5 2] \n",
      "\n",
      "Pre-activation result: [  1 -13]\n",
      "Final result [1 0]\n"
     ]
    }
   ],
   "source": [
    "input_vector = np.array([1, 2, 3])\n",
    "weights_matrix = np.array([[1, -4], [-7, 2], [3, -5]])\n",
    "bias_matrix = np.array([5, 2])\n",
    "\n",
    "\n",
    "print(\"Input:\", input_vector, \"\\n\")\n",
    "print(\"Weights:\\n\", weights_matrix, \"\\n\")\n",
    "print(\"Bias:\", bias_matrix, \"\\n\")\n",
    "\n",
    "result = np.matmul(input_vector, weights_matrix) + bias_matrix\n",
    "\n",
    "print(\"Pre-activation result:\", result)\n",
    "\n",
    "activated_result = np.maximum(result, 0)\n",
    "\n",
    "print(\"Final result:\", activated_result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This final result is the actual result of a 1-layer neural network! Of course, the weights metrix and bias matrix were not trained, so this model can't actually do anything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid\n",
    "\n",
    "This activation function applies a simple function the each element of our vector. \n",
    "\n",
    "![Sigmoid](https://i.stack.imgur.com/SUuRi.png)\n",
    "\n",
    "The resulting output values are guaranteed to be somewhere between 0 and 1. This is the shape of this activation function:\n",
    "\n",
    "![Sigmoid shape](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/320px-Logistic-curve.svg.png)\n",
    "\n",
    "Here's how we apply it to our pre-activated values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [1 2 3] \n",
      "\n",
      "Weights:\n",
      " [[ 1 -4]\n",
      " [-7  2]\n",
      " [ 3 -5]] \n",
      "\n",
      "Bias: [5 2] \n",
      "\n",
      "Pre-activation result: [  1 -13]\n",
      "Final result: [0.7310585786300049, 2.2603242979035746e-06]\n"
     ]
    }
   ],
   "source": [
    "input_vector = np.array([1, 2, 3])\n",
    "weights_matrix = np.array([[1, -4], [-7, 2], [3, -5]])\n",
    "bias_matrix = np.array([5, 2])\n",
    "\n",
    "\n",
    "print(\"Input:\", input_vector, \"\\n\")\n",
    "print(\"Weights:\\n\", weights_matrix, \"\\n\")\n",
    "print(\"Bias:\", bias_matrix, \"\\n\")\n",
    "\n",
    "result = np.matmul(input_vector, weights_matrix) + bias_matrix\n",
    "\n",
    "print(\"Pre-activation result:\", result)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return [1 / (1 + math.exp(-1* val)) for val in x]\n",
    "\n",
    "activated_result = sigmoid(result)\n",
    "\n",
    "print(\"Final result:\", activated_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we now have an untrained 1-layer neural network. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf3.5",
   "language": "python",
   "name": "tf3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
