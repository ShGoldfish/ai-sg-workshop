{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you begin using Keras, import the required modules. If you are having problems with this step, make sure you have installed all dependencies found in requirements.txt and have selected the correct Python kernel (in the top menu, \"Kernel\"->\"Change kernel\"). If you are using virtualenv, your kernel will not show up unless you follow the instructions found [here](https://stackoverflow.com/questions/37891550/jupyter-notebook-running-kernel-in-different-env)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "First, we want to grab the dataset from the CSV file. Load it as a Pandas Dataframe so we can easily work with it in further steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>87930</td>\n",
       "      <td>B</td>\n",
       "      <td>12.47</td>\n",
       "      <td>18.60</td>\n",
       "      <td>81.09</td>\n",
       "      <td>481.9</td>\n",
       "      <td>0.09965</td>\n",
       "      <td>0.1058</td>\n",
       "      <td>0.08005</td>\n",
       "      <td>0.03821</td>\n",
       "      <td>...</td>\n",
       "      <td>24.64</td>\n",
       "      <td>96.05</td>\n",
       "      <td>677.9</td>\n",
       "      <td>0.1426</td>\n",
       "      <td>0.2378</td>\n",
       "      <td>0.2671</td>\n",
       "      <td>0.10150</td>\n",
       "      <td>0.3014</td>\n",
       "      <td>0.08750</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>859575</td>\n",
       "      <td>M</td>\n",
       "      <td>18.94</td>\n",
       "      <td>21.31</td>\n",
       "      <td>123.60</td>\n",
       "      <td>1130.0</td>\n",
       "      <td>0.09009</td>\n",
       "      <td>0.1029</td>\n",
       "      <td>0.10800</td>\n",
       "      <td>0.07951</td>\n",
       "      <td>...</td>\n",
       "      <td>26.58</td>\n",
       "      <td>165.90</td>\n",
       "      <td>1866.0</td>\n",
       "      <td>0.1193</td>\n",
       "      <td>0.2336</td>\n",
       "      <td>0.2687</td>\n",
       "      <td>0.17890</td>\n",
       "      <td>0.2551</td>\n",
       "      <td>0.06589</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>8670</td>\n",
       "      <td>M</td>\n",
       "      <td>15.46</td>\n",
       "      <td>19.48</td>\n",
       "      <td>101.70</td>\n",
       "      <td>748.9</td>\n",
       "      <td>0.10920</td>\n",
       "      <td>0.1223</td>\n",
       "      <td>0.14660</td>\n",
       "      <td>0.08087</td>\n",
       "      <td>...</td>\n",
       "      <td>26.00</td>\n",
       "      <td>124.90</td>\n",
       "      <td>1156.0</td>\n",
       "      <td>0.1546</td>\n",
       "      <td>0.2394</td>\n",
       "      <td>0.3791</td>\n",
       "      <td>0.15140</td>\n",
       "      <td>0.2837</td>\n",
       "      <td>0.08019</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>907915</td>\n",
       "      <td>B</td>\n",
       "      <td>12.40</td>\n",
       "      <td>17.68</td>\n",
       "      <td>81.47</td>\n",
       "      <td>467.8</td>\n",
       "      <td>0.10540</td>\n",
       "      <td>0.1316</td>\n",
       "      <td>0.07741</td>\n",
       "      <td>0.02799</td>\n",
       "      <td>...</td>\n",
       "      <td>22.91</td>\n",
       "      <td>89.61</td>\n",
       "      <td>515.8</td>\n",
       "      <td>0.1450</td>\n",
       "      <td>0.2629</td>\n",
       "      <td>0.2403</td>\n",
       "      <td>0.07370</td>\n",
       "      <td>0.2556</td>\n",
       "      <td>0.09359</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>921385</td>\n",
       "      <td>B</td>\n",
       "      <td>11.54</td>\n",
       "      <td>14.44</td>\n",
       "      <td>74.65</td>\n",
       "      <td>402.9</td>\n",
       "      <td>0.09984</td>\n",
       "      <td>0.1120</td>\n",
       "      <td>0.06737</td>\n",
       "      <td>0.02594</td>\n",
       "      <td>...</td>\n",
       "      <td>19.68</td>\n",
       "      <td>78.78</td>\n",
       "      <td>457.8</td>\n",
       "      <td>0.1345</td>\n",
       "      <td>0.2118</td>\n",
       "      <td>0.1797</td>\n",
       "      <td>0.06918</td>\n",
       "      <td>0.2329</td>\n",
       "      <td>0.08134</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "204   87930         B        12.47         18.60           81.09      481.9   \n",
       "70   859575         M        18.94         21.31          123.60     1130.0   \n",
       "131    8670         M        15.46         19.48          101.70      748.9   \n",
       "431  907915         B        12.40         17.68           81.47      467.8   \n",
       "540  921385         B        11.54         14.44           74.65      402.9   \n",
       "\n",
       "     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "204          0.09965            0.1058         0.08005              0.03821   \n",
       "70           0.09009            0.1029         0.10800              0.07951   \n",
       "131          0.10920            0.1223         0.14660              0.08087   \n",
       "431          0.10540            0.1316         0.07741              0.02799   \n",
       "540          0.09984            0.1120         0.06737              0.02594   \n",
       "\n",
       "        ...       texture_worst  perimeter_worst  area_worst  \\\n",
       "204     ...               24.64            96.05       677.9   \n",
       "70      ...               26.58           165.90      1866.0   \n",
       "131     ...               26.00           124.90      1156.0   \n",
       "431     ...               22.91            89.61       515.8   \n",
       "540     ...               19.68            78.78       457.8   \n",
       "\n",
       "     smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "204            0.1426             0.2378           0.2671   \n",
       "70             0.1193             0.2336           0.2687   \n",
       "131            0.1546             0.2394           0.3791   \n",
       "431            0.1450             0.2629           0.2403   \n",
       "540            0.1345             0.2118           0.1797   \n",
       "\n",
       "     concave points_worst  symmetry_worst  fractal_dimension_worst  \\\n",
       "204               0.10150          0.3014                  0.08750   \n",
       "70                0.17890          0.2551                  0.06589   \n",
       "131               0.15140          0.2837                  0.08019   \n",
       "431               0.07370          0.2556                  0.09359   \n",
       "540               0.06918          0.2329                  0.08134   \n",
       "\n",
       "     Unnamed: 32  \n",
       "204          NaN  \n",
       "70           NaN  \n",
       "131          NaN  \n",
       "431          NaN  \n",
       "540          NaN  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_dataset(path):\n",
    "    dataset = pd.read_csv(path)\n",
    "    np.random.seed(42)\n",
    "    dataset = dataset.reindex(np.random.permutation(dataset.index))\n",
    "    return dataset\n",
    "\n",
    "dataset = get_dataset('data/dataset.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll write functions to get the data and the labels (i.e. the x and the y). We want to make sure these return NumPy arrays so they can be passed into Keras models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30) (569,) [0 1 1 0 0 1 1 1 0 0 0 1 0 1 0 1 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1\n",
      " 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 1 0 0 1 1 0 1\n",
      " 0 0 0 1 0 0 1 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1\n",
      " 0 0 1 0 1 0 0 0 1 0 0 0 1 0 1 1 0 0 1 1 1 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 1\n",
      " 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1\n",
      " 1 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0\n",
      " 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 0 1 0 1 0\n",
      " 1 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0\n",
      " 0 1 0 1 1 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1 1 0 0 1 0 1 1\n",
      " 0 1 1 0 0 1 1 1 0 0 0 0 1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1\n",
      " 0 0 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1\n",
      " 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1\n",
      " 0 0 1 0 0 1 1 1 0 0 0 1 1 0 1 1 0 0 0 1 0 1 1 1 1 0 0 1 0 0 1 1 1 1 1 1 0\n",
      " 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 1 0 0 1 0 0 1 1 1 1 0 0 1\n",
      " 1 0 0 0 1 1 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0\n",
      " 1 1 1 0 1 1 0 1 0 0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "def get_data(dataset):\n",
    "    data = np.array(dataset.as_matrix()[:, 2:-1], dtype=np.float64)\n",
    "    return data\n",
    "\n",
    "def get_labels(dataset):\n",
    "    diagnoses = dataset['diagnosis'].map({'M':1, 'B':0})\n",
    "    return np.array(diagnoses.as_matrix(), dtype=np.uint8)\n",
    "\n",
    "data, labels = get_data(dataset), get_labels(dataset)\n",
    "print(data.shape, labels.shape, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll split the data and labels into train, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(data, labels, split):\n",
    "    train_ratio, val_ratio, test_ratio = split\n",
    "    num_examples = labels.shape[0]\n",
    "    train_bound, val_bound = int(train_ratio*num_examples), int(train_ratio*num_examples) + int(val_ratio*num_examples)\n",
    "    \n",
    "    train = {'data': data[:train_bound], 'labels': labels[:train_bound]}\n",
    "    val = {'data': data[train_bound:val_bound], 'labels': labels[train_bound:val_bound]}\n",
    "    test = {'data': data[val_bound:], 'labels': labels[val_bound:]}\n",
    "    \n",
    "    return train, val, test\n",
    "    \n",
    "train, val, test = split_data(data, labels, (.7, .2, .1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to scale our data so that each feature has mean 0 and variance 1. This is useful because it improves the stability of training our neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(train['data'])\n",
    "train['data'] = scaler.transform(train['data'])\n",
    "val['data'] = scaler.transform(val['data'])\n",
    "test['data'] = scaler.transform(test['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture and Training\n",
    "\n",
    "Let's define our model's architecture first. We want a 4-layer Fully-Connected Network that can be used for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -4.67396006e-01  -1.84719376e-01  -4.45648409e-01  -4.88811444e-01\n",
      "    2.24336174e-01   9.99538139e-03  -1.20505820e-01  -2.84214965e-01\n",
      "    3.95069334e-01   1.20646133e-01  -4.91623988e-02  -3.35921429e-01\n",
      "   -1.96211306e-01  -2.29472298e-01  -3.00198227e-02  -3.65883008e-01\n",
      "   -1.68340364e-01  -2.33546767e-01  -3.33430189e-01  -7.92618563e-02\n",
      "   -2.71967707e-01  -1.98433592e-01  -3.37663286e-01  -3.59263119e-01\n",
      "    4.37063984e-01  -1.20859155e-01  -3.49864647e-02  -2.07715019e-01\n",
      "    1.66114477e-01   1.90793752e-01]\n",
      " [  1.36466458e+00   4.52071175e-01   1.29723979e+00   1.34396332e+00\n",
      "   -4.66534385e-01  -4.43419023e-02   2.25835672e-01   7.76029104e-01\n",
      "   -8.35168558e-01  -1.19078129e+00   1.33935407e+00  -7.85353138e-01\n",
      "    1.25623476e+00   1.17555789e+00  -8.66996586e-01  -5.08614494e-01\n",
      "   -3.06801270e-01   3.07509828e-01  -8.07729182e-01  -8.06112033e-01\n",
      "    1.76216014e+00   1.20689699e-01   1.72448146e+00   1.70723342e+00\n",
      "   -5.86695832e-01  -1.47121176e-01  -2.73955717e-02   9.70532413e-01\n",
      "   -5.74039903e-01  -1.00800638e+00]\n",
      " [  3.79259504e-01   2.20613193e-02   3.99351020e-01   2.66243117e-01\n",
      "    9.14484066e-01   3.19155788e-01   7.04146283e-01   8.10942710e-01\n",
      "    4.16589530e-01  -7.09061790e-01   2.27338716e-01  -8.06502865e-01\n",
      "    9.38891624e-02   1.55543578e-01  -2.67869337e-01  -6.01197080e-01\n",
      "   -1.32443092e-01  -1.42558270e-01  -7.94554210e-01  -5.12369086e-01\n",
      "    6.10378953e-01   2.52817048e-02   5.14060063e-01   4.72309978e-01\n",
      "    9.64322258e-01  -1.10854576e-01   4.96376045e-01   5.51904449e-01\n",
      "   -1.16838709e-01  -2.14723555e-01]\n",
      " [ -4.87217372e-01  -4.00899194e-01  -4.30068604e-01  -5.28685112e-01\n",
      "    6.39870245e-01   4.93409836e-01  -1.53219291e-01  -5.46580446e-01\n",
      "   -1.38143969e-02   1.16892529e+00  -8.24921282e-01   4.22551556e-01\n",
      "   -3.38588923e-01  -5.46971517e-01   9.86428243e-01   3.96821151e-01\n",
      "    5.23964165e-01  -2.23234716e-02   1.51648326e-01   8.52014933e-01\n",
      "   -7.01828901e-01  -4.83012608e-01  -5.27788013e-01  -6.41208316e-01\n",
      "    5.42515639e-01   3.60876858e-02  -1.62133922e-01  -6.30909833e-01\n",
      "   -5.66046875e-01   5.28632383e-01]\n",
      " [ -7.30737017e-01  -1.16222812e+00  -7.09685106e-01  -7.12217100e-01\n",
      "    2.38066865e-01   1.26164746e-01  -2.77629616e-01  -5.99207573e-01\n",
      "    1.12924989e-02   7.08775320e-01  -4.65328399e-01   9.84113285e-01\n",
      "   -6.18484852e-01  -4.30953970e-01   1.70364627e+00   8.47059083e-01\n",
      "    7.45758023e-01   5.08984355e-01  -2.63962155e-01   6.62217721e-01\n",
      "   -8.29347532e-01  -1.01433644e+00  -8.47516397e-01  -7.42089386e-01\n",
      "    8.11646490e-02  -2.83433572e-01  -4.49638995e-01  -6.99717047e-01\n",
      "   -9.28930340e-01  -1.50928082e-01]\n",
      " [  1.83471412e+00   2.33659524e+00   1.97373132e+00   1.72573247e+00\n",
      "    1.53597850e+00   3.21776882e+00   3.24191864e+00   2.63697565e+00\n",
      "    2.08799145e+00   1.04525999e+00   1.11730458e+00   6.68690626e-01\n",
      "    1.39521086e+00   9.65529809e-01  -1.73797019e-01   1.97458272e+00\n",
      "    1.24703778e+00   7.85199433e-01   3.15736614e-01   9.21312090e-01\n",
      "    1.94315433e+00   2.23282529e+00   2.27655171e+00   1.62896363e+00\n",
      "    1.42127943e+00   3.82031988e+00   3.15129087e+00   2.28121851e+00\n",
      "    1.88141826e+00   2.21560657e+00]\n",
      " [  2.23397308e+00   5.90708232e-01   2.26482768e+00   2.33939105e+00\n",
      "    7.04910361e-01   1.68883008e+00   1.92098830e+00   2.58819929e+00\n",
      "    3.28126952e-02  -2.14400567e-01   2.11440579e+00  -9.63302569e-01\n",
      "    2.26454040e+00   1.90520785e+00  -1.02244942e+00   1.35605998e-01\n",
      "    1.12107072e-01   4.71614080e-01  -6.49629518e-01   1.58482658e-03\n",
      "    2.33805073e+00  -7.61760351e-03   2.58358543e+00   2.33513112e+00\n",
      "   -1.42920118e-01   8.21447177e-01   9.54201779e-01   1.94936071e+00\n",
      "   -2.70304844e-01   9.31589419e-02]\n",
      " [  9.76732120e-01  -1.01889150e+00   9.44644198e-01   8.49360159e-01\n",
      "    1.39061356e-01   1.95491626e-01   1.11214606e-01   7.76542539e-01\n",
      "   -2.72056753e-01  -2.01458849e-01   6.72852022e-01  -7.39771828e-01\n",
      "    8.52426568e-01   4.28815113e-01  -4.72359874e-01   2.52436404e-01\n",
      "    5.79406532e-02   2.81513114e-01  -1.62155553e-01  -1.92043204e-02\n",
      "    7.64635362e-01  -1.04065588e+00   8.09284794e-01   5.95802323e-01\n",
      "   -3.14279057e-01   1.50515064e-01  -1.21332873e-01   4.63612006e-01\n",
      "   -2.46325761e-01  -2.70197878e-01]\n",
      " [ -2.21044737e-01  -8.28559269e-01  -2.24251179e-01  -3.81067703e-01\n",
      "    8.13310553e-01   9.03750012e-01   3.36119725e-01   5.28553248e-01\n",
      "    4.56043223e-01   8.81331560e-01  -4.38456168e-01  -3.86972495e-01\n",
      "   -6.63676382e-01  -5.99745614e-01  -8.30605980e-02   5.51125461e-01\n",
      "    2.41593660e-01   3.28632157e-01   8.81689151e-02   3.12267079e-01\n",
      "   -1.56789589e-01  -4.36953576e-01  -3.19654578e-01  -4.68840833e-01\n",
      "    9.20384069e-01   1.38795649e+00   1.00259372e+00   8.47227449e-01\n",
      "    9.86199136e-01   9.72981716e-01]\n",
      " [ -6.24738057e-02  -6.49975941e-01  -1.22572451e-01  -1.56530879e-01\n",
      "   -2.04267318e+00  -9.75757892e-01  -8.36856504e-01  -9.21388277e-01\n",
      "   -6.64099809e-03  -1.08293364e+00  -8.99880663e-01  -1.18883889e+00\n",
      "   -7.33164434e-01  -5.76883961e-01  -1.24595458e+00  -6.94330752e-01\n",
      "   -7.16607090e-01  -1.13335800e+00  -8.91569913e-01  -6.53273304e-01\n",
      "   -2.37002921e-01  -6.65603769e-01  -1.85622550e-01  -2.87950638e-01\n",
      "   -1.70536214e+00  -3.53465628e-01  -6.45578920e-01  -8.05667979e-01\n",
      "   -3.69418390e-01  -3.92796134e-01]] [0 1 1 0 0 1 1 1 0 0]\n",
      "0.9921875\n",
      "[0 1 1 0 0 1 1 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1\n",
      " 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 1 0 0 1 1 0 1\n",
      " 0 0 0 1 0 0 1 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1\n",
      " 0 0 1 0 1 0 0 0 1 0 0 0 1 0 1 1 0 0 1 1 1 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 1\n",
      " 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1\n",
      " 1 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0\n",
      " 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 0 1 0 1 0\n",
      " 1 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0\n",
      " 0 1 0 1 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1 1 0 0 1 0 1 1\n",
      " 0 1 1 0 0 1 1 1 0 0 0 0 1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1\n",
      " 0 0 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1\n",
      " 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1\n",
      " 0 0 1 0 0 1 1 1 0 0 0 1 1 0 1 1 0 0 0 1 0 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 0\n",
      " 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 1 0 0 1 0 0 1]\n",
      "0.964285714286\n",
      "[1 1 1 0 0 1 1 0 0 0 1 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Karan/tf3.5/lib/python3.5/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "print(train['data'][:10], train[\"labels\"][:10])\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# baseline_model = SVC(kernel='poly', degree=2)\n",
    "# baseline_model.fit(train['data'], train['labels'])\n",
    "# print(baseline_model.score(val['data'], val['labels']))\n",
    "\n",
    "baseline_mlp = MLPClassifier()\n",
    "baseline_mlp.fit(train['data'], train['labels'])\n",
    "print(baseline_mlp.score(train['data'], train['labels']))\n",
    "print(baseline_mlp.predict(train['data']))\n",
    "print(baseline_mlp.score(val['data'], val['labels']))\n",
    "print(baseline_mlp.predict(val['data']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_109 (Dense)            (None, 16)                496       \n",
      "_________________________________________________________________\n",
      "dense_110 (Dense)            (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_111 (Dense)            (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_112 (Dense)            (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 673\n",
      "Trainable params: 673\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_model():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, activation='relu', input_dim=30))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(4, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    sgd = SGD(lr=0.001)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train our model! We want to not only train the model with many epochs, but also print the validation set accuracy at every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "512/512 [==============================] - 0s - loss: 7.4927e-04 - acc: 1.0000     \n",
      "Epoch 2/25\n",
      "512/512 [==============================] - 0s - loss: 7.0824e-04 - acc: 1.0000     \n",
      "Epoch 3/25\n",
      "512/512 [==============================] - 0s - loss: 6.8463e-04 - acc: 1.0000     \n",
      "Epoch 4/25\n",
      "512/512 [==============================] - 0s - loss: 6.5316e-04 - acc: 1.0000     \n",
      "Epoch 5/25\n",
      "512/512 [==============================] - 0s - loss: 6.3599e-04 - acc: 1.0000     \n",
      "Epoch 6/25\n",
      "512/512 [==============================] - 0s - loss: 6.4775e-04 - acc: 1.0000     \n",
      "Epoch 7/25\n",
      "512/512 [==============================] - 0s - loss: 5.8751e-04 - acc: 1.0000     \n",
      "Epoch 8/25\n",
      "512/512 [==============================] - 0s - loss: 6.1608e-04 - acc: 1.0000     \n",
      "Epoch 9/25\n",
      "512/512 [==============================] - 0s - loss: 5.6789e-04 - acc: 1.0000     \n",
      "Epoch 10/25\n",
      "512/512 [==============================] - 0s - loss: 5.5468e-04 - acc: 1.0000     \n",
      "Epoch 11/25\n",
      "512/512 [==============================] - 0s - loss: 5.3320e-04 - acc: 1.0000     \n",
      "Epoch 12/25\n",
      "512/512 [==============================] - 0s - loss: 5.0301e-04 - acc: 1.0000     \n",
      "Epoch 13/25\n",
      "512/512 [==============================] - 0s - loss: 4.9857e-04 - acc: 1.0000     \n",
      "Epoch 14/25\n",
      "512/512 [==============================] - 0s - loss: 4.8642e-04 - acc: 1.0000     \n",
      "Epoch 15/25\n",
      "512/512 [==============================] - 0s - loss: 4.6146e-04 - acc: 1.0000     \n",
      "Epoch 16/25\n",
      "512/512 [==============================] - 0s - loss: 4.3956e-04 - acc: 1.0000     \n",
      "Epoch 17/25\n",
      "512/512 [==============================] - 0s - loss: 4.3564e-04 - acc: 1.0000     \n",
      "Epoch 18/25\n",
      "512/512 [==============================] - 0s - loss: 4.3488e-04 - acc: 1.0000     \n",
      "Epoch 19/25\n",
      "512/512 [==============================] - 0s - loss: 4.0948e-04 - acc: 1.0000     \n",
      "Epoch 20/25\n",
      "512/512 [==============================] - 0s - loss: 3.9720e-04 - acc: 1.0000     \n",
      "Epoch 21/25\n",
      "512/512 [==============================] - 0s - loss: 3.8000e-04 - acc: 1.0000     \n",
      "Epoch 22/25\n",
      "512/512 [==============================] - 0s - loss: 3.7253e-04 - acc: 1.0000     \n",
      "Epoch 23/25\n",
      "512/512 [==============================] - 0s - loss: 3.6019e-04 - acc: 1.0000     \n",
      "Epoch 24/25\n",
      "512/512 [==============================] - 0s - loss: 3.4967e-04 - acc: 1.0000     \n",
      "Epoch 25/25\n",
      "512/512 [==============================] - 0s - loss: 3.3338e-04 - acc: 1.0000     \n",
      "[1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "Validation accuracy: 0.93\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, train, val, num_epochs):\n",
    "    model.fit(train['data'], train['labels'], epochs=num_epochs, verbose=1, batch_size=16)\n",
    "    results = model.predict(val['data'])\n",
    "    print([0 if x < .5 else 1 for x in results])\n",
    "    score = model.evaluate(val['data'], val['labels'], batch_size=16, verbose=0)\n",
    "    print(\"Validation accuracy: %.2f\\n\" % (score[1]))\n",
    "        \n",
    "train_model(model, train, val, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Try playing around with hyperparameters like the learning rate, size of the hidden layers, number of epochs, etc. until you get a model that you are satisfied with! Use validation accuracy to compare performance across different model configurations. Once you're done configuring, try testing on a completely unseen dataset to get a good idea of how your model will perform for unseen data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58/58 [==============================] - 0s\n",
      "Test accuracy: 0.36\n"
     ]
    }
   ],
   "source": [
    "test_score = model.evaluate(test['data'], test['labels'], batch_size=128)\n",
    "print(\"Test accuracy: %.2f\" % (test_score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf3.5",
   "language": "python",
   "name": "tf3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
